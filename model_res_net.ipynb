{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/gray_formatted/'\n",
    "batch_size = 16\n",
    "n_workers = 4\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "img_dim = 64\n",
    "display_delay = 500\n",
    "\n",
    "# Model params\n",
    "ds_kernels = [7, 3, 3, 3, 3]\n",
    "ds_filters = [32, 64, 64, 128, 128]\n",
    "\n",
    "n_res_blocks = 6\n",
    "res_kernel = 3\n",
    "res_filter = ds_filters[-1]\n",
    "\n",
    "us_kernels = [3, 3, 3, 3, 7]\n",
    "us_filters = [64, 64, 32, 32, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_transform(img):\n",
    "    img = np.array(img)[:, :, 0]\n",
    "    target, outline = np.split(img, 2, axis=1)\n",
    "    target = transforms.ToTensor()(target)\n",
    "    outline = transforms.ToTensor()(outline)\n",
    "    return outline, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=input_transform)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0)\n",
    "\n",
    "def get_generator():\n",
    "#     data_iter = iter(dataloader)\n",
    "    for (X, y), _ in dataloader:\n",
    "        yield X, y\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # 1 x 64 x 64 input\n",
    "        self.in_filters = 1\n",
    "\n",
    "        ### Downscale ###\n",
    "\n",
    "        self.ds1 = nn.Sequential(\n",
    "            nn.Conv2d(self.in_filters, ds_filters[0], kernel_size=ds_kernels[0],\n",
    "                      stride=1, padding=(ds_kernels[0]-1)//2),\n",
    "            nn.BatchNorm2d(ds_filters[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4))\n",
    "        self.ds2 = nn.Sequential(\n",
    "            nn.Conv2d(ds_filters[0], ds_filters[1], kernel_size=ds_kernels[1],\n",
    "                      stride=2, padding=(ds_kernels[1]-1)//2),\n",
    "            nn.Conv2d(ds_filters[1], ds_filters[2], kernel_size=ds_kernels[2],\n",
    "                      stride=1, padding=(ds_kernels[2]-1)//2),\n",
    "            nn.BatchNorm2d(ds_filters[2]),\n",
    "            nn.ReLU())\n",
    "        self.ds3 = nn.Sequential(\n",
    "            nn.Conv2d(ds_filters[2], ds_filters[3], kernel_size=ds_kernels[3],\n",
    "                      stride=2, padding=(ds_kernels[3]-1)//2),\n",
    "            nn.Conv2d(ds_filters[3], ds_filters[4], kernel_size=ds_kernels[4],\n",
    "                      stride=1, padding=(ds_kernels[4]-1)//2),\n",
    "            nn.BatchNorm2d(ds_filters[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4))\n",
    "        \n",
    "        ### Residual blocks ###\n",
    "\n",
    "        self.res_blocks = []\n",
    "        for _ in range(n_res_blocks):\n",
    "            res_block = nn.Sequential(\n",
    "                nn.Conv2d(res_filter, res_filter, kernel_size=res_kernel,\n",
    "                          stride=1, padding=(res_kernel-1)//2),\n",
    "                nn.BatchNorm2d(res_filter),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(res_filter, res_filter, kernel_size=res_kernel,\n",
    "                          stride=1, padding=(res_kernel-1)//2),\n",
    "                nn.Dropout(0.2))\n",
    "\n",
    "            self.res_blocks.append(res_block)\n",
    "            \n",
    "        ### Upscale ###\n",
    "            \n",
    "        self.us1 = nn.Sequential(\n",
    "            nn.Conv2d(res_filter, us_filters[0], kernel_size=us_kernels[0],\n",
    "                      stride=1, padding=(us_kernels[0]-1)//2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(us_filters[0], us_filters[1], kernel_size=us_kernels[1],\n",
    "                      stride=1, padding=(us_kernels[1]-1)//2),\n",
    "            nn.BatchNorm2d(us_filters[1]),\n",
    "            nn.ReLU())\n",
    "        self.us2 = nn.Sequential(\n",
    "            nn.Conv2d(us_filters[1], us_filters[2], kernel_size=us_kernels[2],\n",
    "                      stride=1, padding=(us_kernels[2]-1)//2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(us_filters[2], us_filters[3], kernel_size=us_kernels[3],\n",
    "                      stride=1, padding=(us_kernels[3]-1)//2),\n",
    "            nn.BatchNorm2d(us_filters[3]),\n",
    "            nn.ReLU())\n",
    "        self.us3 = nn.Conv2d(us_filters[3], us_filters[4], kernel_size=us_kernels[4],\n",
    "            stride=1, padding=(us_kernels[4]-1)//2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.ds1(x)\n",
    "        z2 = self.ds2(z1)\n",
    "        z3 = self.ds3(z2)\n",
    "        \n",
    "        z = z3\n",
    "        for i in range(len(self.res_blocks)):\n",
    "            res_output = self.res_blocks[i](z)\n",
    "            z = z + res_output\n",
    "        \n",
    "        z4 = self.us1(z)\n",
    "        z5 = self.us2(z4)\n",
    "        raw = self.us3(z5)\n",
    "        out = torch.clamp(raw, 0, 1)\n",
    "    \n",
    "        return out\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self = super().to(*args, **kwargs)\n",
    "        for res_block in self.res_blocks:\n",
    "            res_block = res_block.to(*args, **kwargs) \n",
    "        return self\n",
    "\n",
    "def calc_loss(y_pred, y):\n",
    "    loss = (y_pred - y)**2\n",
    "    loss = loss.view(-1)\n",
    "    loss = torch.sum(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_imgs(model, data, temp_set_eval=True):\n",
    "    if temp_set_eval:\n",
    "        model.eval()\n",
    "\n",
    "    X, y = data\n",
    "    example_imgs = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        outline = X[i].squeeze().detach().numpy()\n",
    "        img = model(X[i:i+1].to(device)).detach().squeeze().cpu().numpy()\n",
    "        actual = y[i].squeeze().detach().numpy()\n",
    "\n",
    "        example_imgs.append(outline)\n",
    "        example_imgs.append(img)\n",
    "        example_imgs.append(actual)\n",
    "\n",
    "    if temp_set_eval:\n",
    "        model.train()\n",
    "\n",
    "    return example_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch #1\n",
      "Step #500 loss: 3078.157470703125\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "losses = []\n",
    "img_hist = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Starting epoch #{epoch+1}')\n",
    "    \n",
    "    data_gen = get_generator()\n",
    "    img_hist.append(gen_imgs(model, next(data_gen)))\n",
    "    for step, (X, y) in enumerate(data_gen):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = calc_loss(y_pred, y)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        if step > 0 and step % display_delay == 0:\n",
    "            print(f'Step #{step} loss: {np.mean(losses[-display_delay:])}')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch #{epoch+1} loss: {np.mean(losses[-step:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/resnet_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_gen = get_generator()\n",
    "example_imgs = gen_imgs(model, next(data_gen))\n",
    "\n",
    "plt.figure(figsize=(12, batch_size * 5))\n",
    "columns = 3\n",
    "for i, img in enumerate(example_imgs):\n",
    "    plt.subplot(len(example_imgs) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_drawn = []\n",
    "for img_path in glob.glob('../data/handdrawn/*.png'):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = torch.tensor(img[:, :, 0].transpose(2, 0, 1))\n",
    "    \n",
    "data = torch.stack(hand_drawn)\n",
    "    \n",
    "example_imgs = gen_imgs(model, next(data_gen))\n",
    "\n",
    "plt.figure(figsize=(8, batch_size * 5))\n",
    "columns = 2\n",
    "for i, img in enumerate(example_imgs):\n",
    "    plt.subplot(len(example_imgs) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread('../data/handdrawn/img3.png').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_drawn = []\n",
    "for img_path in glob.glob('../data/handdrawn/*.png'):\n",
    "    img = np.float32(cv2.imread(img_path)) / 255.\n",
    "    img = torch.tensor(img[:, :, :1]).permute((2, 0, 1))\n",
    "    img = img.to(device)\n",
    "    hand_drawn.append(img)\n",
    "\n",
    "hd_data = torch.stack(hand_drawn)\n",
    "hd_out = model(hd_data).detach().numpy()\n",
    "\n",
    "all_imgs = []\n",
    "for img1, img2 in zip(hd_data, hd_out):\n",
    "    all_imgs.append(img1.squeeze().detach().numpy())\n",
    "    all_imgs.append(img2.squeeze())\n",
    "    \n",
    "plt.figure(figsize=(8, len(all_imgs) * 5 / 2))\n",
    "columns = 2\n",
    "for i, img in enumerate(all_imgs):\n",
    "    plt.subplot(len(all_imgs) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(img, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
